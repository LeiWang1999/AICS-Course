### 6.1 深度学习处理器和20世纪80年代的早期神经网络芯片有何区别？

20世纪80年代早期的神经网络芯片只能处理很小规模的浅层神经网络算法，并且没有取得工业实践中的广泛应用。首先，当时的神经网络因为计算能力的限制没有发展到深层的神经网络，也没有针对多层神经网络的训练算法，加之当时的主流集成电路工艺还是1微米的工艺（当前主流集成电路的工艺已达到0.007微米），无法用有限的硬件神经元支持大规模的算法神经元。

而深度学习处理器不受神经网络规模限制，可以灵活、高效地处理上百层、千万神经元、上亿突触的各种深度学习神经网络（甚至更大）。

### 6.2 假设存在一个深度学习处理器，相较于CPU，它只能加速卷积层10倍，加速全连接层2倍，其他层不加速，那么对于AlexNet它整体的加速比是多少？

我们假设在ALexNet在CPU上，卷积层耗时$t_c$.全连接层耗时$t_f$。则在加速器上，卷积层耗时为$\frac{t_c}{10}$,全连接层耗时为$\frac{t_f}{2}$,则加速比为：
$$
Rate = \frac{t_c+t_f}{\frac{t_c}{10} + \frac{t_f}{2}}
$$

### 6.3 假设设计一个深度学习处理器，它通过PCIe和DDR3的内存相连接。假设带宽为12.8Gb/s，那么和只有一个ALU的深度学习处理器相比，最理想情况下全连接层的加速比能有多少？

最理想的情况下，一层全连接层刚好有12.8Gb的权重，进行全连接层运算的时候，12.8Gb的权重一次性从DDR3中读入，然后并行运算得到结果，而ALU只能一次读入一组数据进行运算，加速比为12.8G倍。

### 6.4 简述为什么指令级并行在深度学习处理器中一般情况下作用不大。什么情况下指令集并行也能在深度学习处理器中发挥作用？

指令集并行的优点在于灵活性高，缺点在于指令流水线的控制逻辑复杂，功耗、面积开销都很大。由于深度学习中主要是规整的向量、矩阵操作（尤其是卷积层和全连接层），行为可控，且深度学习处理去迫切需要提升效率，降低功耗及面积开销，因此深度学习处理器应尽可能简化控制通路，不适合采用复杂的流水线结构。

当深度学习算法的加速，涉及到复杂的控制逻辑的时候，指令集并行能够发挥作用。

### 6.5 请设计一个深度学习专用的指令集，并采用所设计的指令集实现一个卷积神经网络。

### 6.6 请针对习题6.5中设计的指令集，写出其解码单元的工作逻辑（可以画状态转移图表示）。

### 6.7 假设采用习题6.6中的解码单元，请问每条指令的负载是否平衡？如果不平衡应当怎么改进？请针对改进后的指令集或者工作逻辑重新完成习题6.5和习题6.6。

### 6.8 对于本章所设计的DLP，假设片内带宽给定为12.8GB/s（忽略片外带宽限制），请以AlexNet为例，计算出其计算单元的利用率（非空置时间），并给出利用率随着计算单元大小改变的变化，并请画图表示。

### 6.9 其他条件同习题6.8，假设采用6.4.1节中基于标量MAC的计算架构，请重新计算上提。

### 6.10 Roofline模型是一种常用的评估硬件能力的方法，请针对本章所设计的DLP画出Roofline图。

### 6.11 对于一个卷积层，请写出其循环表示的代码，并给出至少三种不同的分块方法（代码），请画图表示你提出的三种不同分块所对应的计算过程。

### 6.12 习题6.11中的循环表示方式，对于深度学习处理器设计来说有什么不足？应该如何改进？请你提出一种能更好地被深度学习处理器设计所使用的新的卷积表示方法，并解释其原因。

### 6.13 请针对本章的DLP设计并实现一个周期精准的模拟器，要求至少支持三个不同类型的网络。请将三个网络的数据标注在习题6.10所完成的Roofline图上。